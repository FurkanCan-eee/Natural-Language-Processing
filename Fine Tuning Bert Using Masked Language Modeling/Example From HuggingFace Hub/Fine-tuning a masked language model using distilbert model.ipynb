{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d68cee",
   "metadata": {},
   "source": [
    "# Fine-Tuning A Masked Language Model Using Distilbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687eec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfecaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482a312",
   "metadata": {},
   "source": [
    "We can see how many parameters this model has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab399f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923be1b",
   "metadata": {},
   "source": [
    "With around 67 million parameters, DistilBERT is approximately two times smaller than the BERT base model, which roughly translates into a two-fold speedup in training. Let’s now see what kinds of tokens this model predicts are the most likely completions of a small sample of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712de50",
   "metadata": {},
   "source": [
    "To predict the mask we need DistilBERT’s tokenizer to produce the inputs for the model, so let’s download that from the Hub as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bfce5",
   "metadata": {},
   "source": [
    "With a tokenizer and a model, we can now pass our text example to the model, extract the logits, and print out the top 5 candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57214651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7037f",
   "metadata": {},
   "source": [
    "We can see from the outputs that the model’s predictions refer to everyday terms, which is perhaps not surprising given the foundation of English Wikipedia. Let’s see how we can change this domain to something a bit more niche — highly polarized movie reviews! \n",
    "\n",
    "To showcase domain adaptation, we’ll use the famous Large Movie Review Dataset (or IMDb for short), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models. By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5494640",
   "metadata": {},
   "source": [
    "Let’s take a look at a few samples to get an idea of what kind of text we’re dealing with. As we’ve done in previous chapters of the course, we’ll chain the Dataset.shuffle() and Dataset.select() functions to create a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec02958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafda84",
   "metadata": {},
   "source": [
    "So to get started, we’ll first tokenize our corpus as usual, but without setting the truncation=True option in our tokenizer. We’ll also grab the word IDs if they are available (which they will be if we’re using a fast tokenizer), as we will need them later on to do whole word masking. We’ll wrap this in a simple function, and while we’re at it we’ll remove the text and label columns since we don’t need them any longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63450aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38573546",
   "metadata": {},
   "source": [
    "To see what the model’s maximum context size is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8780b",
   "metadata": {},
   "source": [
    "So, in order to run our experiments on GPUs (like those found on Google Colab), we’ll pick something a bit smaller that can fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2e367",
   "metadata": {},
   "source": [
    "To show how the concatenation works, let’s take a few reviews from our tokenized training set and print out the number of tokens per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a7e1c",
   "metadata": {},
   "source": [
    "We can then concatenate all these examples with a simple dictionary comprehension, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5307f73",
   "metadata": {},
   "source": [
    "The total length checks out — so now let’s split the concatenated reviews into chunks of the size given by block_size. To do so, we iterate over the features in concatenated_examples and use a list comprehension to create slices of each feature. The result is a dictionary of chunks for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf1df2",
   "metadata": {},
   "source": [
    "The last chunk will generally be smaller than the maximum chunk size. There are two main strategies for dealing with this:\n",
    "\n",
    "- Drop the last chunk if it’s smaller than chunk_size.\n",
    "- Pad the last chunk until its length equals chunk_size.\n",
    "\n",
    "We’ll take the first approach here, so let’s wrap all of the above logic in a single function that we can apply to our tokenized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa7c85",
   "metadata": {},
   "source": [
    "Note that in the last step of group_texts() we create a new labels column which is a copy of the input_ids one. As we’ll see shortly, that’s because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a labels column we provide the ground truth for our language model to learn from.\n",
    "\n",
    "Let’s now apply group_texts() to our tokenized datasets using our trusty Dataset.map() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27739c77",
   "metadata": {},
   "source": [
    "You can see that grouping and then chunking the texts has produced many more examples than our original 25,000 for the train and test splits. That’s because we now have examples involving contiguous tokens that span across multiple examples from the original corpus. You can see this explicitly by looking for the special [SEP] and [CLS] tokens in one of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda037bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f89b09",
   "metadata": {},
   "source": [
    "In this example you can see two overlapping movie reviews, one about a high school movie and the other about homelessness. Let’s also check out what the labels look like for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d85308",
   "metadata": {},
   "source": [
    "As expected from our group_texts() function above, this looks identical to the decoded input_ids — but then how can our model possibly learn anything? We’re missing a key step: inserting [MASK] tokens at random positions in the inputs! Let’s see how we can do this on the fly during fine-tuning using a special data collator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68e122",
   "metadata": {},
   "source": [
    "### Fine-tuning DistilBERT with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40f465",
   "metadata": {},
   "source": [
    "Fine-tuning a masked language model is almost identical to fine-tuning a sequence classification model, like we did in Chapter 3. The only difference is that we need a special data collator that can randomly mask some of the tokens in each batch of texts. Fortunately, Transformers comes prepared with a dedicated DataCollatorForLanguageModeling for just this task. We just have to pass it the tokenizer and an mlm_probability argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16fa3d",
   "metadata": {},
   "source": [
    "To see how the random masking works, let’s feed a few examples to the data collator. Since it expects a list of dicts, where each dict represents a single chunk of contiguous text, we first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key for this data collator as it does not expect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637ed39",
   "metadata": {},
   "source": [
    "Nice, it worked! We can see that the [MASK] token has been randomly inserted at various locations in our text. These will be the tokens which our model will have to predict during training — and the beauty of the data collator is that it will randomize the [MASK] insertion with every batch!\n",
    "\n",
    "When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called whole word masking. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let’s do this now! We’ll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all -100 except for the ones corresponding to mask words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afe83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1726fb2",
   "metadata": {},
   "source": [
    "Next, we can try it on the same samples as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ebed4",
   "metadata": {},
   "source": [
    "Now that we have two data collators, the rest of the fine-tuning steps are standard. We’ll first downsample the size of the training set to a few thousand examples. Don’t worry, we’ll still get a pretty decent language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e47386",
   "metadata": {},
   "source": [
    "The next thing we need to do is log in to the Hugging Face Hub. If you’re running this code in a notebook, you can do so with the following utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce197a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e90ac",
   "metadata": {},
   "source": [
    "We can specify the arguments for the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f0695",
   "metadata": {},
   "source": [
    "Here we tweaked a few of the default options, including logging_steps to ensure we track the training loss with each epoch. We’ve also used fp16=True to enable mixed-precision training, which gives us another boost in speed. By default, the Trainer will remove any columns that are not part of the model’s forward() method. This means that if you’re using the whole word masking collator, you’ll also need to set remove_unused_columns=False to ensure we don’t lose the word_ids column during training.\n",
    "\n",
    "We now have all the ingredients to instantiate the Trainer. Here we just use the standard data_collator, but you can try the whole word masking collator and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2edcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4193f9f",
   "metadata": {},
   "source": [
    "We’re now ready to run trainer.train() — but before doing so let’s briefly look at perplexity, which is a common metric to evaluate the performance of language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a945870",
   "metadata": {},
   "source": [
    "A lower perplexity score means a better language model, and we can see here that our starting model has a somewhat large value. Let’s see if we can lower it by fine-tuning! To do that, we first run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed4edf",
   "metadata": {},
   "source": [
    "and then compute the resulting perplexity on the test set as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebbb6d",
   "metadata": {},
   "source": [
    "Nice — this is quite a reduction in perplexity, which tells us the model has learned something about the domain of movie reviews!\n",
    "\n",
    "Once training is finished, we can push the model card with the training information to the Hub (the checkpoints are saved during training itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc2716",
   "metadata": {},
   "source": [
    "In our use case we didn’t need to do anything special with the training loop, but in some cases you might need to implement some custom logic.\n",
    "\n",
    "You can interact with your fine-tuned model either by using its widget on the Hub or locally with the pipeline from Transformers. Let’s use the latter to download our model using the fill-mask pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb759bd7",
   "metadata": {},
   "source": [
    "We can then feed the pipeline our sample text of “This is a great [MASK]” and see what the top 5 predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b423ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d0390",
   "metadata": {},
   "source": [
    "Our model has clearly adapted its weights to predict words that are more strongly associated with movies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
